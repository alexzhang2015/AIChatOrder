"""
æ€§èƒ½è¯„åˆ†å™¨
Performance Graders

åŒ…å«å»¶è¿Ÿè¯„åˆ†å™¨å’ŒåŸºå‡†å¯¹æ¯”è¯„åˆ†å™¨
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import yaml

from evals.graders.base import BaseGrader
from evals.harness.models import GraderResult, GraderConfig, TestCase, GraderType


@dataclass
class LatencyThresholds:
    """å»¶è¿Ÿé˜ˆå€¼é…ç½®"""
    p50_target: float = 500
    p95_target: float = 1000
    p99_target: float = 2000
    critical: float = 3000


class LatencyGrader(BaseGrader):
    """
    å»¶è¿Ÿæ€§èƒ½è¯„åˆ†å™¨

    è¯„ä¼°ç³»ç»Ÿå“åº”å»¶è¿Ÿæ˜¯å¦æ»¡è¶³ SLA è¦æ±‚

    é…ç½®ç¤ºä¾‹:
    ```yaml
    graders:
      - type: latency
        p50_target: 500
        p95_target: 1000
        p99_target: 2000
        critical: 3000
    ```
    """

    grader_type = GraderType.LATENCY

    def __init__(self, config: Optional[GraderConfig] = None):
        super().__init__(config)
        self.thresholds = LatencyThresholds(
            p50_target=config.p50_target if config and config.p50_target else 500,
            p95_target=config.p95_target if config and config.p95_target else 1000,
            p99_target=config.p99_target if config and config.p99_target else 2000,
            critical=config.critical if config and config.critical else 3000
        )
        self.component_filter = config.component if config else None

    def evaluate(
        self,
        predictions: List[Dict[str, Any]],
        test_cases: List[TestCase],
        transcript: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> GraderResult:
        """
        è¯„ä¼°å»¶è¿Ÿæ€§èƒ½

        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœï¼ˆåº”åŒ…å« latency_ms å­—æ®µï¼‰
            test_cases: æµ‹è¯•ç”¨ä¾‹
            transcript: å¯¹è¯è®°å½•

        Returns:
            GraderResult
        """
        # æ”¶é›†å»¶è¿Ÿæµ‹é‡
        latency_measurements = []
        for pred in predictions:
            latency = pred.get("latency_ms") or pred.get("duration_ms", 0)
            if latency > 0:
                latency_measurements.append(latency)

        if not latency_measurements:
            return GraderResult(
                grader_type=self.grader_type,
                passed=True,
                score=1.0,
                details={"note": "No latency data available"},
                failures=[]
            )

        # è®¡ç®—ç™¾åˆ†ä½æ•°
        sorted_latencies = sorted(latency_measurements)
        n = len(sorted_latencies)

        def percentile(p: float) -> float:
            idx = (n - 1) * p / 100
            lower = int(idx)
            upper = min(lower + 1, n - 1)
            weight = idx - lower
            return sorted_latencies[lower] * (1 - weight) + sorted_latencies[upper] * weight

        p50 = percentile(50)
        p90 = percentile(90)
        p95 = percentile(95)
        p99 = percentile(99)

        # æ£€æŸ¥è¿è§„
        violations = []
        failures = []
        score = 100.0

        if p50 > self.thresholds.p50_target:
            ratio = p50 / self.thresholds.p50_target
            deduction = min(30, (ratio - 1) * 30)
            score -= deduction
            violations.append({
                "type": "p50_violation",
                "actual": round(p50, 2),
                "target": self.thresholds.p50_target,
                "deduction": round(deduction, 1)
            })
            failures.append({
                "type": "p50_violation",
                "message": f"P50 å»¶è¿Ÿ {p50:.1f}ms è¶…è¿‡ç›®æ ‡ {self.thresholds.p50_target}ms"
            })

        if p95 > self.thresholds.p95_target:
            ratio = p95 / self.thresholds.p95_target
            deduction = min(40, (ratio - 1) * 40)
            score -= deduction
            violations.append({
                "type": "p95_violation",
                "actual": round(p95, 2),
                "target": self.thresholds.p95_target,
                "deduction": round(deduction, 1)
            })
            failures.append({
                "type": "p95_violation",
                "message": f"P95 å»¶è¿Ÿ {p95:.1f}ms è¶…è¿‡ç›®æ ‡ {self.thresholds.p95_target}ms"
            })

        if p99 > self.thresholds.p99_target:
            ratio = p99 / self.thresholds.p99_target
            deduction = min(30, (ratio - 1) * 30)
            score -= deduction
            violations.append({
                "type": "p99_violation",
                "actual": round(p99, 2),
                "target": self.thresholds.p99_target,
                "deduction": round(deduction, 1)
            })
            failures.append({
                "type": "p99_violation",
                "message": f"P99 å»¶è¿Ÿ {p99:.1f}ms è¶…è¿‡ç›®æ ‡ {self.thresholds.p99_target}ms"
            })

        # æ£€æŸ¥ä¸¥é‡è¶…æ—¶
        max_latency = max(sorted_latencies)
        if max_latency > self.thresholds.critical:
            failures.append({
                "type": "critical_breach",
                "message": f"æœ€å¤§å»¶è¿Ÿ {max_latency:.1f}ms è¶…è¿‡ä¸¥é‡é˜ˆå€¼ {self.thresholds.critical}ms"
            })

        score = max(0, score) / 100

        # ç¡®å®šç­‰çº§
        if score >= 0.95:
            grade = "A"
        elif score >= 0.90:
            grade = "B"
        elif score >= 0.85:
            grade = "C"
        elif score >= 0.80:
            grade = "D"
        else:
            grade = "F"

        passed = len(violations) == 0 or score >= 0.80

        return GraderResult(
            grader_type=self.grader_type,
            passed=passed,
            score=score,
            details={
                "grade": grade,
                "statistics": {
                    "count": n,
                    "p50": round(p50, 2),
                    "p90": round(p90, 2),
                    "p95": round(p95, 2),
                    "p99": round(p99, 2),
                    "min": round(min(sorted_latencies), 2),
                    "max": round(max(sorted_latencies), 2),
                    "mean": round(sum(sorted_latencies) / n, 2)
                },
                "thresholds": {
                    "p50_target": self.thresholds.p50_target,
                    "p95_target": self.thresholds.p95_target,
                    "p99_target": self.thresholds.p99_target,
                    "critical": self.thresholds.critical
                },
                "violations": violations
            },
            failures=failures
        )


class BenchmarkGrader(BaseGrader):
    """
    è¡Œä¸šåŸºå‡†å¯¹æ¯”è¯„åˆ†å™¨

    å°†è¯„ä¼°ç»“æœä¸è¡Œä¸šåŸºå‡†è¿›è¡Œå¯¹æ¯”

    é…ç½®ç¤ºä¾‹:
    ```yaml
    graders:
      - type: benchmark
        metric: "intent_accuracy"
        benchmark_source: "industry"
    ```
    """

    grader_type = GraderType.BENCHMARK

    def __init__(self, config: Optional[GraderConfig] = None):
        super().__init__(config)
        self.metric = config.metric if config and config.metric else "intent_accuracy"
        self.benchmark_source = config.benchmark_source if config and config.benchmark_source else "industry"
        self.benchmarks = self._load_benchmarks()

    def _load_benchmarks(self) -> Dict[str, Any]:
        """åŠ è½½åŸºå‡†é…ç½®"""
        path = Path(__file__).parent.parent / "metrics" / "industry_benchmarks.yaml"
        if path.exists():
            with open(path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        return {}

    def _get_benchmark_value(self, metric: str) -> Optional[Dict[str, float]]:
        """è·å–æŒ‡å®šæŒ‡æ ‡çš„åŸºå‡†å€¼"""
        if metric in ["intent_accuracy", "intent_recognition"]:
            ir = self.benchmarks.get("intent_recognition", {})
            overall = ir.get("overall_accuracy", {})
            return {
                "excellent": overall.get("excellent", 0.98),
                "good": overall.get("good", 0.95),
                "acceptable": overall.get("acceptable", 0.90),
                "industry_average": overall.get("industry_average", 0.92)
            }

        if metric in ["slot_f1", "slot_extraction"]:
            se = self.benchmarks.get("slot_extraction", {})
            overall = se.get("overall_f1", {})
            return {
                "excellent": overall.get("excellent", 0.95),
                "good": overall.get("good", 0.90),
                "acceptable": overall.get("acceptable", 0.85),
                "industry_average": overall.get("industry_average", 0.88)
            }

        if metric in ["task_completion", "task_completion_rate"]:
            dm = self.benchmarks.get("dialogue_management", {})
            tcr = dm.get("task_completion_rate", {})
            return {
                "excellent": tcr.get("excellent", 0.92),
                "good": tcr.get("good", 0.85),
                "acceptable": tcr.get("acceptable", 0.80),
                "industry_average": tcr.get("industry_average", 0.83)
            }

        if metric in ["fuzzy_accuracy", "fuzzy_understanding"]:
            fu = self.benchmarks.get("fuzzy_understanding", {})
            overall = fu.get("overall_accuracy", {})
            return {
                "excellent": overall.get("excellent", 0.90),
                "good": overall.get("good", 0.85),
                "acceptable": overall.get("acceptable", 0.80),
                "industry_average": overall.get("industry_average", 0.82)
            }

        return None

    def evaluate(
        self,
        predictions: List[Dict[str, Any]],
        test_cases: List[TestCase],
        transcript: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> GraderResult:
        """
        è¯„ä¼°å¹¶ä¸åŸºå‡†å¯¹æ¯”

        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            test_cases: æµ‹è¯•ç”¨ä¾‹

        Returns:
            GraderResult
        """
        # æ ¹æ®æŒ‡æ ‡è®¡ç®—å®é™…å€¼
        actual_value = self._calculate_metric(predictions, test_cases)

        benchmark = self._get_benchmark_value(self.metric)

        if not benchmark:
            return GraderResult(
                grader_type=self.grader_type,
                passed=True,
                score=actual_value,
                details={"note": f"No benchmark available for {self.metric}"},
                failures=[]
            )

        # ç¡®å®šç­‰çº§
        if actual_value >= benchmark["excellent"]:
            grade = "A"
            grade_label = "ä¼˜ç§€"
            emoji = "ğŸŸ¢"
        elif actual_value >= benchmark["good"]:
            grade = "B"
            grade_label = "è‰¯å¥½"
            emoji = "ğŸŸ¡"
        elif actual_value >= benchmark["acceptable"]:
            grade = "C"
            grade_label = "åˆæ ¼"
            emoji = "ğŸŸ "
        else:
            grade = "D"
            grade_label = "å¾…æ”¹è¿›"
            emoji = "ğŸ”´"

        # ä¸è¡Œä¸šå¹³å‡å¯¹æ¯”
        vs_industry = actual_value - benchmark["industry_average"]
        vs_industry_pct = (vs_industry / benchmark["industry_average"]) * 100 if benchmark["industry_average"] > 0 else 0

        passed = actual_value >= benchmark["acceptable"]
        failures = []
        if not passed:
            gap = benchmark["acceptable"] - actual_value
            failures.append({
                "type": "below_benchmark",
                "message": f"{self.metric} ({actual_value:.2%}) ä½äºè¡Œä¸šå¯æ¥å—æ ‡å‡† ({benchmark['acceptable']:.2%})ï¼Œå·®è· {gap:.2%}"
            })

        interpretation = self._generate_interpretation(actual_value, benchmark, grade)

        return GraderResult(
            grader_type=self.grader_type,
            passed=passed,
            score=actual_value,
            details={
                "metric": self.metric,
                "grade": grade,
                "grade_label": f"{emoji} {grade_label}",
                "benchmark_comparison": {
                    "actual": round(actual_value, 4),
                    "industry_average": benchmark["industry_average"],
                    "vs_industry": f"{vs_industry_pct:+.1f}%",
                    "excellent_threshold": benchmark["excellent"],
                    "good_threshold": benchmark["good"],
                    "acceptable_threshold": benchmark["acceptable"]
                },
                "interpretation": interpretation
            },
            failures=failures
        )

    def _calculate_metric(
        self,
        predictions: List[Dict[str, Any]],
        test_cases: List[TestCase]
    ) -> float:
        """è®¡ç®—æŒ‡å®šæŒ‡æ ‡çš„å€¼"""
        if self.metric in ["intent_accuracy", "intent_recognition"]:
            # è®¡ç®—æ„å›¾å‡†ç¡®ç‡
            if not predictions or not test_cases:
                return 0.0
            correct = 0
            total = 0
            for pred, tc in zip(predictions, test_cases):
                if tc.expected_intent:
                    total += 1
                    if pred.get("intent") == tc.expected_intent:
                        correct += 1
            return correct / total if total > 0 else 0.0

        if self.metric in ["slot_f1", "slot_extraction"]:
            # ç®€åŒ–çš„ F1 è®¡ç®—
            if not predictions or not test_cases:
                return 0.0
            tp = fp = fn = 0
            for pred, tc in zip(predictions, test_cases):
                pred_slots = pred.get("slots", {})
                expected_slots = tc.expected_slots or {}
                for key in set(pred_slots.keys()) | set(expected_slots.keys()):
                    if key in pred_slots and key in expected_slots:
                        if pred_slots[key] == expected_slots[key]:
                            tp += 1
                        else:
                            fp += 1
                            fn += 1
                    elif key in pred_slots:
                        fp += 1
                    else:
                        fn += 1
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

        return 0.0

    def _generate_interpretation(
        self,
        actual: float,
        benchmark: Dict[str, float],
        grade: str
    ) -> str:
        """ç”Ÿæˆè§£è¯»è¯´æ˜"""
        metric_names = {
            "intent_accuracy": "æ„å›¾è¯†åˆ«å‡†ç¡®ç‡",
            "slot_f1": "æ§½ä½æå– F1",
            "task_completion": "ä»»åŠ¡å®Œæˆç‡",
            "fuzzy_accuracy": "æ¨¡ç³Šè¡¨è¾¾ç†è§£å‡†ç¡®ç‡"
        }
        metric_name = metric_names.get(self.metric, self.metric)

        if grade == "A":
            return f"{metric_name}è¾¾åˆ°ä¼˜ç§€æ°´å¹³ï¼Œè¶…è¶Šè¡Œä¸šé¢†å…ˆæ ‡å‡†"
        elif grade == "B":
            return f"{metric_name}è¾¾åˆ°è‰¯å¥½æ°´å¹³ï¼Œé«˜äºè¡Œä¸šå¹³å‡"
        elif grade == "C":
            return f"{metric_name}è¾¾åˆ°åˆæ ¼æ°´å¹³ï¼Œæ¥è¿‘è¡Œä¸šå¹³å‡"
        else:
            gap = benchmark["acceptable"] - actual
            return f"{metric_name}ä½äºè¡Œä¸šæ ‡å‡†ï¼Œéœ€æå‡ {gap:.1%} è¾¾åˆ°åˆæ ¼çº¿"


class PerformanceProfileGrader(BaseGrader):
    """
    æ€§èƒ½ç”»åƒè¯„åˆ†å™¨

    ç»¼åˆè¯„ä¼°å»¶è¿Ÿå’Œå‡†ç¡®ç‡ï¼Œç”Ÿæˆæ€§èƒ½ç”»åƒ

    é…ç½®ç¤ºä¾‹:
    ```yaml
    graders:
      - type: performance_profile
        latency_weight: 0.3
        accuracy_weight: 0.7
    ```
    """

    grader_type = GraderType.PERFORMANCE_PROFILE

    def __init__(self, config: Optional[GraderConfig] = None):
        super().__init__(config)
        self.latency_weight = config.latency_weight if config and config.latency_weight else 0.3
        self.accuracy_weight = config.accuracy_weight if config and config.accuracy_weight else 0.7

    def evaluate(
        self,
        predictions: List[Dict[str, Any]],
        test_cases: List[TestCase],
        transcript: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> GraderResult:
        """
        ç”Ÿæˆæ€§èƒ½ç”»åƒ

        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            test_cases: æµ‹è¯•ç”¨ä¾‹

        Returns:
            GraderResult
        """
        # æ”¶é›†å»¶è¿Ÿæ•°æ®
        latencies = []
        for pred in predictions:
            latency = pred.get("latency_ms") or pred.get("duration_ms", 0)
            if latency > 0:
                latencies.append(latency)

        # è®¡ç®—å‡†ç¡®ç‡
        correct = 0
        total = 0
        for pred, tc in zip(predictions, test_cases):
            if tc.expected_intent:
                total += 1
                if pred.get("intent") == tc.expected_intent:
                    correct += 1

        accuracy_score = correct / total if total > 0 else 0.0

        # è®¡ç®—å»¶è¿Ÿè¯„åˆ† (è¶Šä½è¶Šå¥½)
        latency_score = 1.0
        latency_stats = {}
        if latencies:
            sorted_lat = sorted(latencies)
            n = len(sorted_lat)
            p50 = sorted_lat[n // 2]
            p95_idx = min(int(n * 0.95), n - 1)
            p95 = sorted_lat[p95_idx]
            # å‡è®¾ 1000ms ä¸ºåŸºå‡†
            latency_score = max(0, 1 - (p95 - 500) / 1500)
            latency_stats = {
                "count": n,
                "p50": round(p50, 2),
                "p95": round(p95, 2),
                "min": round(min(sorted_lat), 2),
                "max": round(max(sorted_lat), 2),
                "score": round(latency_score, 3)
            }

        accuracy_stats = {
            "correct": correct,
            "total": total,
            "accuracy": round(accuracy_score, 4),
            "score": round(accuracy_score, 3)
        }

        # ç»¼åˆè¯„åˆ†
        overall_score = (
            self.latency_weight * latency_score +
            self.accuracy_weight * accuracy_score
        )

        # ç¡®å®šæ€§èƒ½ç”»åƒç±»å‹
        profile = self._determine_profile(latency_score, accuracy_score)

        passed = overall_score >= 0.80
        failures = []
        if not passed:
            if latency_score < 0.7:
                failures.append({
                    "type": "high_latency",
                    "message": f"å»¶è¿Ÿè¯„åˆ† {latency_score:.2f} è¿‡ä½"
                })
            if accuracy_score < 0.85:
                failures.append({
                    "type": "low_accuracy",
                    "message": f"å‡†ç¡®ç‡ {accuracy_score:.2%} éœ€æå‡"
                })

        return GraderResult(
            grader_type=self.grader_type,
            passed=passed,
            score=overall_score,
            details={
                "profile": profile,
                "latency": latency_stats,
                "accuracy": accuracy_stats,
                "weights": {
                    "latency": self.latency_weight,
                    "accuracy": self.accuracy_weight
                },
                "overall_score": round(overall_score, 3)
            },
            failures=failures
        )

    def _determine_profile(self, latency_score: float, accuracy_score: float) -> Dict[str, str]:
        """ç¡®å®šæ€§èƒ½ç”»åƒç±»å‹"""
        if latency_score >= 0.9 and accuracy_score >= 0.95:
            return {
                "type": "é«˜æ€§èƒ½",
                "emoji": "ğŸš€",
                "description": "å»¶è¿Ÿä½ã€å‡†ç¡®ç‡é«˜ï¼Œè¡¨ç°ä¼˜å¼‚"
            }
        elif latency_score >= 0.8 and accuracy_score >= 0.90:
            return {
                "type": "å‡è¡¡å‹",
                "emoji": "âš–ï¸",
                "description": "å»¶è¿Ÿå’Œå‡†ç¡®ç‡å¹³è¡¡è‰¯å¥½"
            }
        elif latency_score < 0.7 and accuracy_score >= 0.90:
            return {
                "type": "å‡†ç¡®ä¼˜å…ˆ",
                "emoji": "ğŸ¯",
                "description": "å‡†ç¡®ç‡é«˜ä½†å»¶è¿Ÿè¾ƒå¤§ï¼Œé€‚åˆå¯¹å‡†ç¡®æ€§è¦æ±‚é«˜çš„åœºæ™¯"
            }
        elif latency_score >= 0.8 and accuracy_score < 0.85:
            return {
                "type": "é€Ÿåº¦ä¼˜å…ˆ",
                "emoji": "âš¡",
                "description": "å“åº”å¿«ä½†å‡†ç¡®ç‡éœ€æå‡"
            }
        else:
            return {
                "type": "å¾…ä¼˜åŒ–",
                "emoji": "ğŸ”§",
                "description": "å»¶è¿Ÿå’Œå‡†ç¡®ç‡éƒ½éœ€è¦æ”¹è¿›"
            }
