"""
è¯„ä¼°æ¡†æ¶ CLI å…¥å£

ç”¨æ³•:
    # è¿è¡Œå›å½’æµ‹è¯•å¥—ä»¶
    python -m evals.run --suite regression

    # è¿è¡Œèƒ½åŠ›æµ‹è¯•å¥—ä»¶
    python -m evals.run --suite capability --trials 5

    # è¿è¡Œå•ä¸ªä»»åŠ¡
    python -m evals.run --task intent/basic_intent_classification.yaml

    # è¿è¡Œæ‰€æœ‰ä»»åŠ¡
    python -m evals.run --suite all --verbose

    # è®¾ç½®å¤±è´¥é˜ˆå€¼
    python -m evals.run --suite regression --fail-threshold 0.95
"""

import sys
import argparse
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from evals.harness.runner import EvalRunner
from evals.harness.reporter import EvalReporter


def setup_logging(verbose: bool = False):
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%H:%M:%S"
    )


def main():
    parser = argparse.ArgumentParser(
        description="AI ç‚¹å•ç³»ç»Ÿè¯„ä¼°æ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m evals.run --suite regression
  python -m evals.run --task intent/basic_intent_classification.yaml
  python -m evals.run --suite all --verbose --report markdown
        """
    )

    # ä»»åŠ¡é€‰æ‹©
    task_group = parser.add_mutually_exclusive_group(required=True)
    task_group.add_argument(
        "--suite",
        choices=["regression", "capability", "edge_case", "all"],
        help="è¿è¡Œè¯„ä¼°å¥—ä»¶"
    )
    task_group.add_argument(
        "--task",
        type=str,
        help="è¿è¡Œå•ä¸ªä»»åŠ¡ (ç›¸å¯¹äº evals/tasks çš„è·¯å¾„)"
    )

    # æ‰§è¡Œå‚æ•°
    parser.add_argument(
        "--trials",
        type=int,
        default=3,
        help="æ¯ä¸ªä»»åŠ¡çš„è¯•éªŒæ¬¡æ•° (é»˜è®¤: 3)"
    )
    parser.add_argument(
        "--fail-threshold",
        type=float,
        default=0.0,
        help="å¤±è´¥é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¿”å›éé›¶é€€å‡ºç  (é»˜è®¤: 0)"
    )

    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º"
    )
    parser.add_argument(
        "--report",
        choices=["none", "json", "markdown", "both"],
        default="json",
        help="æŠ¥å‘Šæ ¼å¼ (é»˜è®¤: json)"
    )
    parser.add_argument(
        "--report-only",
        action="store_true",
        help="ä»…ç”ŸæˆæŠ¥å‘Šï¼Œä¸é˜»å¡ CI (å³ä½¿å¤±è´¥ä¹Ÿè¿”å› 0)"
    )

    # ç›®å½•å‚æ•°
    parser.add_argument(
        "--tasks-dir",
        type=str,
        default="evals/tasks",
        help="ä»»åŠ¡ç›®å½• (é»˜è®¤: evals/tasks)"
    )
    parser.add_argument(
        "--results-dir",
        type=str,
        default="evals/results",
        help="ç»“æœç›®å½• (é»˜è®¤: evals/results)"
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)

    # åˆ›å»ºæ‰§è¡Œå™¨å’ŒæŠ¥å‘Šå™¨
    runner = EvalRunner(
        tasks_dir=args.tasks_dir,
        results_dir=args.results_dir
    )
    reporter = EvalReporter(output_dir=args.results_dir)

    exit_code = 0

    try:
        if args.suite:
            # è¿è¡Œå¥—ä»¶
            print(f"\nğŸš€ å¼€å§‹è¿è¡Œè¯„ä¼°å¥—ä»¶: {args.suite}")
            print(f"   è¯•éªŒæ¬¡æ•°: {args.trials}")
            print(f"   å¤±è´¥é˜ˆå€¼: {args.fail_threshold:.0%}")
            print()

            result = runner.run_suite(
                suite_name=args.suite,
                num_trials=args.trials,
                fail_threshold=args.fail_threshold,
                save_transcript=True
            )

            # æ‰“å°ç»“æœ
            reporter.print_suite_result(result, verbose=args.verbose)

            # ç”ŸæˆæŠ¥å‘Š
            if args.report in ["json", "both"]:
                json_path = reporter.generate_json_report(result)
                print(f"\nğŸ“„ JSON æŠ¥å‘Š: {json_path}")

            if args.report in ["markdown", "both"]:
                md_path = reporter.generate_markdown_report(result)
                print(f"ğŸ“„ Markdown æŠ¥å‘Š: {md_path}")

            # æ£€æŸ¥é˜ˆå€¼
            if args.fail_threshold > 0 and result.overall_pass_rate < args.fail_threshold:
                if not args.report_only:
                    exit_code = 1
                    print(f"\nâŒ è¯„ä¼°å¤±è´¥: é€šè¿‡ç‡ {result.overall_pass_rate:.1%} < é˜ˆå€¼ {args.fail_threshold:.1%}")
                else:
                    print(f"\nâš ï¸ é€šè¿‡ç‡ä½äºé˜ˆå€¼ï¼Œä½†ä½¿ç”¨äº† --report-onlyï¼Œä¸é˜»å¡")

        else:
            # è¿è¡Œå•ä¸ªä»»åŠ¡
            print(f"\nğŸš€ å¼€å§‹è¿è¡Œä»»åŠ¡: {args.task}")
            print(f"   è¯•éªŒæ¬¡æ•°: {args.trials}")
            print()

            task = runner.load_task(args.task)
            result = runner.run_task(task, num_trials=args.trials)

            # æ‰“å°ç»“æœ
            reporter.print_task_result(result, verbose=args.verbose)

            if result.pass_all < 1.0 and not args.report_only:
                exit_code = 1

    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        exit_code = 1
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        exit_code = 1

    print()
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
