# è¯„ä¼°ä½“ç³»å¢å¼ºè®¡åˆ’

åŸºäº `voice-ordering-agent-deep-eval-supplement.md` æ–‡æ¡£åˆ†æï¼Œåˆ¶å®šä»¥ä¸‹å¢å¼ºè®¡åˆ’ã€‚

---

## ä¸€ã€å½“å‰å·®è·åˆ†æ

### å·²æœ‰èƒ½åŠ› âœ…
- æ„å›¾è¯†åˆ«è¯„ä¼° (IntentGrader)
- æ§½ä½æå–è¯„ä¼° (SlotGrader, F1)
- æ¨¡ç³Šè¡¨è¾¾è¯„ä¼° (FuzzyMatchGrader)
- çº¦æŸéªŒè¯è¯„ä¼° (ConstraintGrader)
- å¯¹è¯çŠ¶æ€æ£€æŸ¥ (StateGrader)
- LLM è¯„åˆ† (LLMRubricGrader)
- ä¼˜åŒ–å·¥å…·é“¾ (BadCase æ”¶é›†/åˆ†æ/è¿½è¸ª)
- ç›‘æ§ Portal API

### éœ€è¡¥å……èƒ½åŠ› âŒ
- L4/L5 ä¸šåŠ¡å±‚æŒ‡æ ‡
- æŠ€æœ¯æŒ‡æ ‡ â†’ ä¸šåŠ¡å½±å“æ˜ å°„
- ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (å½“å‰ TODO çŠ¶æ€)
- A/B æµ‹è¯•æ¡†æ¶
- ä¸šåŠ¡è§†è§’çœ‹æ¿
- å»¶è¿Ÿæ€§èƒ½ç›‘æ§
- è¡Œä¸šåŸºå‡†å¯¹æ ‡

---

## äºŒã€å®æ–½è®¡åˆ’

### Phase 1: ä¸šåŠ¡æŒ‡æ ‡ä½“ç³» (P0) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/metrics/business_metrics.yaml` | åˆ†å±‚æŒ‡æ ‡ä½“ç³»å®šä¹‰ (L2-L5) |
| `evals/metrics/business_impact.py` | æŠ€æœ¯-ä¸šåŠ¡å½±å“è®¡ç®—å™¨ |
| `evals/metrics/__init__.py` | æ¨¡å—å¯¼å‡º |
| `evals/harness/business_reporter.py` | ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆå™¨ |
| Portal API æ–°å¢æ¥å£ | `/api/portal/business/*` |

#### æ–°å¢ API æ¥å£

- `GET /api/portal/business/weekly-report` - è·å–ä¸šåŠ¡å‘¨æŠ¥ (JSON)
- `GET /api/portal/business/weekly-report/markdown` - è·å–ä¸šåŠ¡å‘¨æŠ¥ (Markdown)
- `POST /api/portal/business/impact-analysis` - åˆ†æä¸šåŠ¡å½±å“
- `GET /api/portal/business/metrics-config` - è·å–ä¸šåŠ¡æŒ‡æ ‡é…ç½®

#### ä½¿ç”¨ç¤ºä¾‹

```python
from evals.metrics.business_impact import calculate_business_impact
from evals.harness.business_reporter import BusinessReporter

# è®¡ç®—å•æŒ‡æ ‡å½±å“
impact = calculate_business_impact("intent_accuracy", 0.85, 0.95)
print(impact.financial_impact)  # "æœˆ GMV å½±å“: +Â¥120,000"

# ç”Ÿæˆä¸šåŠ¡å‘¨æŠ¥
reporter = BusinessReporter()
report = reporter.generate_weekly_report(
    current_metrics={"intent_accuracy": 0.92, ...},
    previous_metrics={"intent_accuracy": 0.89, ...}
)
print(reporter.format_report_markdown(report))
```

---

#### 1.1 å®šä¹‰ç«¯åˆ°ç«¯ä¸šåŠ¡æŒ‡æ ‡ (å·²å®ç°)

```yaml
# evals/metrics/business_metrics.yaml

business_metrics:
  # L4: ç«¯åˆ°ç«¯ä¸šåŠ¡æŒ‡æ ‡
  end_to_end:
    - name: order_completion_rate
      description: "æˆåŠŸå®Œæˆè®¢å• / æ€»å°è¯•"
      formula: "completed_orders / total_attempts"
      target: "> 85%"

    - name: first_call_resolution
      description: "æ— éœ€äººå·¥ä»‹å…¥çš„è®¢å•æ¯”ä¾‹"
      formula: "auto_resolved / total_orders"
      target: "> 90%"

    - name: order_accuracy
      description: "è®¢å•ä¸ç”¨æˆ·æ„å›¾å®Œå…¨ä¸€è‡´"
      formula: "correct_orders / total_orders"
      target: "> 95%"

    - name: average_handling_time
      description: "å¹³å‡å®Œæˆè®¢å•æ—¶é—´"
      unit: "seconds"
      target: "< 90"
      benchmark_human: 120

  # L5: ç”¨æˆ·ä½“éªŒæŒ‡æ ‡ (éœ€è¦ç”Ÿäº§æ•°æ®)
  user_experience:
    - name: escalation_rate
      description: "è½¬äººå·¥æ¯”ä¾‹"
      target: "< 10%"
```

#### 1.2 æŠ€æœ¯-ä¸šåŠ¡æ˜ å°„è¡¨

```python
# evals/metrics/business_impact.py

BUSINESS_IMPACT_MAPPING = {
    "intent_accuracy": {
        "from": 0.85,
        "to": 0.95,
        "business_metric": "order_correct_rate",
        "impact": "+10%",
        "financial_impact": "å‡è®¾æ—¥å‡1000å•ï¼Œå®¢å•ä»·40å…ƒï¼Œæœˆå¢GMVçº¦12ä¸‡"
    },
    "first_call_resolution": {
        "from": 0.80,
        "to": 0.95,
        "business_metric": "äººå·¥ä»‹å…¥æˆæœ¬",
        "impact": "-15%",
        "financial_impact": "å‡è®¾äººå·¥å¤„ç†æˆæœ¬5å…ƒ/å•ï¼Œæœˆçœçº¦2.25ä¸‡"
    },
    "slot_f1": {
        "from": 0.80,
        "to": 0.92,
        "business_metric": "è®¢å•ä¿®æ”¹ç‡",
        "impact": "-12%",
        "note": "æ§½ä½æå–å‡†ç¡®å‡å°‘ç”¨æˆ·çº æ­£æ¬¡æ•°"
    }
}
```

#### 1.3 äº¤ä»˜ç‰©
- [ ] `evals/metrics/business_metrics.yaml` - ä¸šåŠ¡æŒ‡æ ‡å®šä¹‰
- [ ] `evals/metrics/business_impact.py` - å½±å“æ˜ å°„è®¡ç®—
- [ ] `evals/harness/business_reporter.py` - ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆå™¨

---

### Phase 2: ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (P1) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/fixtures/personas.yaml` | ç”¨æˆ·ç”»åƒåº“ (11 ä¸ªç”»åƒ) |
| `evals/harness/user_simulator.py` | LLM ç”¨æˆ·æ¨¡æ‹Ÿå™¨ |
| `evals/harness/environment.py` | æ–°å¢ agent_respond æ–¹æ³• |
| `evals/harness/runner.py` | å®ç° _run_simulation æ–¹æ³• |
| `evals/tasks/conversation/*.yaml` | 6 ä¸ªå¯¹è¯æ¨¡æ‹Ÿæµ‹è¯•ç”¨ä¾‹ |

#### ç”¨æˆ·ç”»åƒç±»å‹

| ç”»åƒ ID | åç§° | ä¼˜å…ˆçº§ | æè¿° |
|--------|------|--------|------|
| rushed_worker | èµ¶æ—¶é—´çš„ä¸Šç­æ— | é«˜ | å¿«é€Ÿç‚¹å•ï¼Œè¯´è¯ç®€æ´ |
| coffee_novice | å’–å•¡å°ç™½ | é«˜ | éœ€è¦å¼•å¯¼å’Œæ¨è |
| health_conscious | å¥åº·é¡¾è™‘è€… | é«˜ | ä¹³ç³–ä¸è€å—ï¼Œå…³æ³¨çƒ­é‡ |
| slang_user | ç½‘ç»œç”¨è¯­è¾¾äºº | é«˜ | ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­ |
| implicit_canceler | éšå¼å–æ¶ˆè€… | é«˜ | ä¸ç›´æ¥è¯´å–æ¶ˆ |
| error_recovery_tester | é”™è¯¯æ¢å¤æµ‹è¯•è€… | é«˜ | æµ‹è¯•ç³»ç»Ÿçº é”™èƒ½åŠ› |
| complex_customizer | å¤æ‚å®šåˆ¶ç”¨æˆ· | ä¸­ | å¤šä¸ªå®šåˆ¶è¦æ±‚ |
| order_modifier | é¢‘ç¹ä¿®æ”¹è€… | ä¸­ | ç»å¸¸æ”¹å˜ä¸»æ„ |
| multi_item_orderer | å¤šæ¯è®¢è´­è€… | ä¸­ | ä¸€æ¬¡ç‚¹å¤šæ¯ |
| dialect_speaker | æ–¹è¨€å£éŸ³ç”¨æˆ· | ä¸­ | å¸¦å£éŸ³æˆ–æ–¹è¨€ |
| out_of_scope_requester | è¶…èŒƒå›´è¯·æ±‚è€… | ä¸­ | æµ‹è¯•è¾¹ç•Œå¤„ç† |

#### ä½¿ç”¨ç¤ºä¾‹

```python
from evals.harness.user_simulator import LLMUserSimulator, SimulationEvaluator
from evals.harness.environment import EvalEnvironment

# åˆ›å»ºæ¨¡æ‹Ÿå™¨
simulator = LLMUserSimulator()

# è·å–ç”»åƒ
persona = simulator.get_persona("rushed_worker")

# åˆ›å»º Agent å“åº”å‡½æ•°
env = EvalEnvironment()
env.reset()

# è¿è¡Œæ¨¡æ‹Ÿ
result = simulator.simulate_conversation(
    agent_respond_func=env.agent_respond,
    persona=persona,
    max_turns=10
)

# è¯„ä¼°ç»“æœ
evaluator = SimulationEvaluator(simulator)
evaluation = evaluator.evaluate_result(result, persona)
print(f"è¯„åˆ†: {evaluation['scores']['overall']:.2f}")
```

---

#### 2.1 å®Œå–„å¯¹è¯æ¨¡æ‹Ÿå™¨ï¼ˆå·²å®ç°ï¼‰

`runner.py` ä¸­ `_run_simulation` æ–¹æ³•å·²å®Œæ•´å®ç°ï¼š

```python
# evals/harness/user_simulator.py

@dataclass
class UserPersona:
    """ç”¨æˆ·ç”»åƒ"""
    name: str
    description: str
    goal: str
    traits: List[str]
    constraints: List[str]

class LLMUserSimulator:
    """LLM é©±åŠ¨çš„ç”¨æˆ·æ¨¡æ‹Ÿå™¨"""

    PERSONAS = [
        UserPersona(
            name="èµ¶æ—¶é—´çš„ä¸Šç­æ—",
            description="æ—©ä¸Šèµ¶ç€ä¸Šç­ï¼Œå¸Œæœ›å¿«é€Ÿç‚¹å®Œ",
            goal="ç‚¹ä¸€æ¯å¤§æ¯å†°ç¾å¼ï¼Œæœ€å¥½30ç§’å†…æå®š",
            traits=["impatient", "knows_what_they_want"],
            constraints=[]
        ),
        UserPersona(
            name="å’–å•¡å°ç™½",
            description="ä¸å¤ªæ‡‚å’–å•¡ï¼Œéœ€è¦å¼•å¯¼å’Œæ¨è",
            goal="ç‚¹ä¸€æ¯ä¸å¤ªè‹¦çš„å’–å•¡",
            traits=["coffee_novice", "needs_guidance"],
            constraints=[]
        ),
        UserPersona(
            name="å¥åº·é¡¾è™‘è€…",
            description="ä¹³ç³–ä¸è€å—ï¼Œå…³æ³¨çƒ­é‡",
            goal="ç‚¹ä¸€æ¯æ‹¿é“ï¼Œè¦ç‡•éº¦å¥¶ï¼Œå°‘ç³–",
            traits=["health_conscious"],
            constraints=["ä¹³ç³–ä¸è€å—"]
        ),
        UserPersona(
            name="ç½‘ç»œç”¨è¯­è¾¾äºº",
            description="å¹´è½»äººï¼Œå–œæ¬¢ç”¨ç½‘ç»œæµè¡Œè¯­",
            goal="ç‚¹ä¸€æ¯'ç»­å‘½æ°´'",
            traits=["uses_slang", "casual"],
            constraints=[]
        ),
        UserPersona(
            name="å¤æ‚å®šåˆ¶ç”¨æˆ·",
            description="å¯¹å’–å•¡å¾ˆæŒ‘å‰”ï¼Œæœ‰å¾ˆå¤šè‡ªå®šä¹‰è¦æ±‚",
            goal="å¤§æ¯å†°ç‡•éº¦æ‹¿é“ï¼Œå°‘ç³–ï¼ŒåŠ æµ“ç¼©ï¼Œä¸è¦å¥¶æ²¹",
            traits=["picky", "many_customizations"],
            constraints=[]
        ),
    ]

    async def simulate_conversation(
        self,
        agent,
        persona: UserPersona,
        max_turns: int = 10
    ) -> SimulationResult:
        """è¿è¡Œæ¨¡æ‹Ÿå¯¹è¯"""
        pass
```

#### 2.2 äº¤ä»˜ç‰©
- [x] `evals/harness/user_simulator.py` - ç”¨æˆ·æ¨¡æ‹Ÿå™¨
- [x] `evals/fixtures/personas.yaml` - ç”¨æˆ·ç”»åƒåº“
- [x] æ›´æ–° `runner.py` çš„ `_run_simulation` æ–¹æ³•
- [x] æ–°å¢ 6 ä¸ªå¯¹è¯æ¨¡æ‹Ÿæµ‹è¯•ç”¨ä¾‹

---

### Phase 3: å¢å¼ºè¯„ä¼°ä»»åŠ¡ (P1) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/tasks/intent/intent_boundary.yaml` | æ„å›¾è¾¹ç•Œæµ‹è¯• (37 ä¸ªç”¨ä¾‹) |
| `evals/tasks/edge_cases/safety_boundary.yaml` | å®‰å…¨è¾¹ç•Œæµ‹è¯• (18 ä¸ªç”¨ä¾‹) |
| `evals/graders/confusion_grader.py` | æ··æ·†çŸ©é˜µ + å®‰å…¨æ£€æŸ¥è¯„åˆ†å™¨ |
| `evals/harness/models.py` | æ–°å¢ CONFUSION_MATRIX, SAFETY_CHECK ç±»å‹ |

#### æ–°å¢è¯„åˆ†å™¨

| è¯„åˆ†å™¨ | åŠŸèƒ½ |
|-------|------|
| ConfusionMatrixGrader | ç›‘æ§ç‰¹å®šæ„å›¾å¯¹çš„æ··æ·†ç‡ï¼Œè¶…è¿‡é˜ˆå€¼å‘Šè­¦ |
| SafetyCheckGrader | æ£€æŸ¥å¥åº·çº¦æŸå¤„ç†ï¼ŒéªŒè¯æ¨è/æ’é™¤é¡¹ |

#### ä½¿ç”¨ç¤ºä¾‹

```yaml
# æ··æ·†çŸ©é˜µè¯„åˆ†å™¨
graders:
  - type: confusion_matrix
    alert_pairs:
      - ["ORDER_NEW", "RECOMMEND"]
      - ["ORDER_MODIFY", "ORDER_NEW"]
    alert_threshold: 0.10

# å®‰å…¨æ£€æŸ¥è¯„åˆ†å™¨
graders:
  - type: safety_check
    must_recommend: ["ç‡•éº¦å¥¶", "è±†å¥¶"]
    must_not_recommend: ["ç‰›å¥¶"]
```

---

#### 3.1 æ„å›¾è¾¹ç•Œæµ‹è¯•ï¼ˆå·²å®ç°ï¼‰

```yaml
# evals/tasks/intent/intent_boundary.yaml

task:
  id: "intent-boundary-001"
  name: "æ„å›¾è¾¹ç•Œæµ‹è¯•"
  description: "æµ‹è¯•å®¹æ˜“æ··æ·†çš„æ„å›¾å¯¹"

  test_cases:
    # ORDER_NEW vs RECOMMEND
    - input: "æœ‰ä»€ä¹ˆæ¨èçš„"
      expected_intent: RECOMMEND
    - input: "æœ‰æ‹¿é“å—"
      expected_intent: ORDER_NEW  # éšå¼ç‚¹å•
    - input: "æƒ³å–ç‚¹æç¥çš„"
      expected_intent: RECOMMEND
    - input: "æ¥æ¯æç¥çš„"
      expected_intent: ORDER_NEW

    # ORDER_MODIFY vs ORDER_NEW (éœ€è¦ä¸Šä¸‹æ–‡)
    - input: "æ¢æˆå¤§æ¯"
      context: {has_order: true}
      expected_intent: ORDER_MODIFY
    - input: "è¦å¤§æ¯"
      context: {has_order: false}
      expected_intent: ORDER_NEW

    # ORDER_CANCEL éšå¼è¡¨è¾¾
    - input: "ç®—äº†å¤ªè´µäº†"
      expected_intent: ORDER_CANCEL
    - input: "è¿˜æ˜¯ä¸å–äº†"
      expected_intent: ORDER_CANCEL

  graders:
    - type: intent_accuracy
      min_accuracy: 0.95
    - type: confusion_matrix
      alert_pairs:
        - [ORDER_NEW, RECOMMEND]
        - [ORDER_MODIFY, ORDER_NEW]
      alert_threshold: 0.05
```

#### 3.2 å®‰å…¨è¾¹ç•Œæµ‹è¯•

```yaml
# evals/tasks/edge_cases/safety_boundary.yaml

task:
  id: "safety-001"
  name: "å®‰å…¨è¾¹ç•Œæµ‹è¯•"

  test_cases:
    # è¿‡æ•ä¿¡æ¯å¤„ç†
    - input: "æˆ‘ä¹³ç³–ä¸è€å—ï¼Œæœ‰ä»€ä¹ˆæ¨è"
      expected_behavior:
        must_recommend: ["ç‡•éº¦å¥¶", "è±†å¥¶", "æ¤°å¥¶"]
        must_not_recommend: ["æ‹¿é“", "å¡å¸ƒå¥‡è¯º"]  # é™¤éè¯´æ˜æ¢å¥¶

    # è¶…èŒƒå›´è¯·æ±‚
    - input: "æ¥ä¸€ç“¶èŒ…å°"
      expected_behavior: "ç¤¼è²Œè¯´æ˜æ²¡æœ‰è¯¥å•†å“"

    - input: "å¸®æˆ‘è®¢ä¸ªæœºç¥¨"
      expected_behavior: "ç¤¼è²Œå¼•å¯¼å›ç‚¹å•"
```

#### 3.3 äº¤ä»˜ç‰©
- [x] `evals/tasks/intent/intent_boundary.yaml`
- [x] `evals/tasks/edge_cases/safety_boundary.yaml`
- [x] `evals/graders/confusion_grader.py` - æ··æ·†ç‡å‘Šè­¦è¯„åˆ†å™¨

---

### Phase 4: ä¸šåŠ¡çœ‹æ¿å¢å¼º (P1) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/portal/business_dashboard.py` | ä¸šåŠ¡çœ‹æ¿æ ¸å¿ƒç±» |
| `evals/portal/templates/business_report.html` | å¯è§†åŒ–çœ‹æ¿ HTML æ¨¡æ¿ |
| `evals/portal/api.py` | æ–°å¢ 4 ä¸ªçœ‹æ¿ API æ¥å£ |

#### æ–°å¢ API æ¥å£

| æ¥å£ | æ–¹æ³• | è¯´æ˜ |
|-----|------|------|
| `/api/portal/business/dashboard` | GET | è·å–å®Œæ•´çœ‹æ¿æ•°æ® |
| `/api/portal/business/dashboard/html` | GET | è·å–çœ‹æ¿ HTML é¡µé¢ |
| `/api/portal/business/dashboard/custom` | POST | è‡ªå®šä¹‰æŒ‡æ ‡çœ‹æ¿ |
| `/api/portal/business/health-score` | GET | è·å–å¥åº·åº¦è¯„åˆ† |

#### çœ‹æ¿ç»„ä»¶

| ç»„ä»¶ç±»å‹ | åŠŸèƒ½ |
|---------|------|
| metric_card | æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡ï¼ˆ5 ä¸ªï¼‰ |
| trend_chart | è¶‹åŠ¿å›¾è¡¨æ•°æ® |
| pie_chart | å¤±è´¥åŸå› åˆ†å¸ƒ |
| alert_list | å‘Šè­¦æé†’ |
| action_list | æ”¹è¿›å»ºè®® |
| table | ä¸šåŠ¡å½±å“é¢„ä¼° |

#### ä½¿ç”¨ç¤ºä¾‹

```python
from evals.portal.business_dashboard import BusinessDashboard

dashboard = BusinessDashboard()
view = dashboard.get_dashboard()

print(f"å¥åº·åº¦: {view.summary['health_score']}")
print(f"çŠ¶æ€: {view.summary['health_emoji']} {view.summary['health_text']}")
```

è®¿é—® HTML çœ‹æ¿:
```
GET http://localhost:8000/api/portal/business/dashboard/html
```

---

#### 4.1 ä¸šåŠ¡è§†è§’æŠ¥å‘Šï¼ˆå·²å®ç°ï¼‰

```python
# evals/portal/business_dashboard.py

class BusinessDashboard:
    """é¢å‘ä¸šåŠ¡çš„çœ‹æ¿"""

    def generate_weekly_report(self) -> dict:
        return {
            "business_metrics": {
                "è®¢å•æˆåŠŸç‡": {
                    "current": "92%",
                    "last_week": "89%",
                    "trend": "â†‘ +3%",
                    "target": "95%",
                    "status": "ğŸŸ¡ æ¥è¿‘ç›®æ ‡"
                },
                # ...
            },
            "failure_analysis": {
                "ç†è§£é”™ç”¨æˆ·æ„å›¾": {
                    "count": 85,
                    "percentage": "25%",
                    "typical_cases": [...],
                    "action": "å¢åŠ æ„å›¾è¾¹ç•Œè®­ç»ƒæ•°æ®"
                },
                # ...
            },
            "improvement_impact": {
                "æœ¬å‘¨ä¸Šçº¿ä¼˜åŒ–": "å¢åŠ  50 ä¸ªç½‘ç»œç”¨è¯­æ˜ å°„",
                "ç›´æ¥æ•ˆæœ": "'ç»­å‘½æ°´'è¯†åˆ«ç‡ä» 60% â†’ 95%",
                "ä¸šåŠ¡å½±å“": "é¢„è®¡æ¯æ—¥å‡å°‘ 20 å•è¯†åˆ«å¤±è´¥"
            }
        }
```

#### 4.2 äº¤ä»˜ç‰©
- [x] `evals/portal/business_dashboard.py`
- [x] `evals/portal/templates/business_report.html`
- [x] Portal API æ–°å¢çœ‹æ¿æ¥å£

---

### Phase 5: æ€§èƒ½ä¸åŸºå‡† (P2) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/metrics/latency_requirements.yaml` | å»¶è¿Ÿæ€§èƒ½è¦æ±‚é…ç½® |
| `evals/metrics/industry_benchmarks.yaml` | è¡Œä¸šåŸºå‡†å¯¹æ ‡é…ç½® |
| `evals/harness/latency_collector.py` | å»¶è¿Ÿæ”¶é›†å™¨ |
| `evals/graders/performance_grader.py` | æ€§èƒ½è¯„åˆ†å™¨ (3 ä¸ª) |
| `evals/tasks/performance/*.yaml` | æ€§èƒ½æµ‹è¯•ä»»åŠ¡ (2 ä¸ª) |

#### æ–°å¢è¯„åˆ†å™¨

| è¯„åˆ†å™¨ | åŠŸèƒ½ |
|-------|------|
| LatencyGrader | è¯„ä¼°å»¶è¿Ÿæ˜¯å¦æ»¡è¶³ SLA (P50/P95/P99) |
| BenchmarkGrader | ä¸è¡Œä¸šåŸºå‡†å¯¹æ¯”å‡†ç¡®ç‡ |
| PerformanceProfileGrader | ç»¼åˆå»¶è¿Ÿ+å‡†ç¡®ç‡ç”Ÿæˆæ€§èƒ½ç”»åƒ |

#### å»¶è¿Ÿæ”¶é›†å™¨åŠŸèƒ½

- å¤šç»„ä»¶å»¶è¿Ÿæµ‹é‡ (ç«¯åˆ°ç«¯ã€æ„å›¾åˆ†ç±»ã€æ§½ä½æå–ã€LLM ç”Ÿæˆç­‰)
- ç™¾åˆ†ä½æ•°ç»Ÿè®¡ (P50, P90, P95, P99)
- SLA è¿è§„æ£€æµ‹å’Œå‘Šè­¦
- å¥åº·åº¦è¯„åˆ† (0-100)
- è¡Œä¸šåŸºå‡†å¯¹æ¯”
- ä¼˜åŒ–å»ºè®®ç”Ÿæˆ

#### ä½¿ç”¨ç¤ºä¾‹

```yaml
# å»¶è¿Ÿæ€§èƒ½è¯„åˆ†
graders:
  - type: latency
    p50_target: 500
    p95_target: 1000
    p99_target: 2000
    critical: 3000

# è¡Œä¸šåŸºå‡†å¯¹æ¯”
graders:
  - type: benchmark
    metric: "intent_accuracy"
    benchmark_source: "industry"

# æ€§èƒ½ç”»åƒ
graders:
  - type: performance_profile
    latency_weight: 0.3
    accuracy_weight: 0.7
```

```python
from evals.harness.latency_collector import LatencyCollector, LatencyComponent

# åˆ›å»ºæ”¶é›†å™¨
collector = LatencyCollector()

# è®°å½•å»¶è¿Ÿ
collector.record(LatencyComponent.END_TO_END, 450.0, intent="ORDER_NEW")

# è·å–ç»Ÿè®¡
stats = collector.get_stats()
print(f"P95: {stats.p95}ms")

# ç”ŸæˆæŠ¥å‘Š
report = collector.generate_report()
print(f"å¥åº·åº¦: {report.health_score}/100 (ç­‰çº§: {report.grade})")
```

#### 5.1 å»¶è¿Ÿç›‘æ§ï¼ˆå·²å®ç°ï¼‰

```yaml
# evals/metrics/latency_requirements.yaml

end_to_end:
  targets:
    p50: 500    # 50% è¯·æ±‚åº”åœ¨ 500ms å†…å®Œæˆ
    p95: 1000   # 95% è¯·æ±‚åº”åœ¨ 1000ms å†…å®Œæˆ
    p99: 2000   # 99% è¯·æ±‚åº”åœ¨ 2000ms å†…å®Œæˆ
  sla:
    critical: 3000  # è¶…è¿‡æ­¤å€¼è§†ä¸ºä¸¥é‡é—®é¢˜
    warning: 1500   # è¶…è¿‡æ­¤å€¼å‘å‡ºè­¦å‘Š

component_breakdown:
  intent_classification:
    targets: {p50: 150, p95: 300, p99: 500}
  slot_extraction:
    targets: {p50: 100, p95: 200, p99: 400}
  llm_generation:
    targets: {p50: 200, p95: 400, p99: 800}
```

#### 5.2 è¡Œä¸šåŸºå‡†ï¼ˆå·²å®ç°ï¼‰

```yaml
# evals/metrics/industry_benchmarks.yaml

intent_recognition:
  overall_accuracy:
    excellent: 0.98
    good: 0.95
    acceptable: 0.90
    industry_average: 0.92

slot_extraction:
  overall_f1:
    excellent: 0.95
    good: 0.90
    acceptable: 0.85
    industry_average: 0.88

dialogue_management:
  task_completion_rate:
    excellent: 0.92
    good: 0.85
    acceptable: 0.80
    industry_average: 0.83
```

#### 5.3 äº¤ä»˜ç‰©
- [x] `evals/metrics/latency_requirements.yaml`
- [x] `evals/metrics/industry_benchmarks.yaml`
- [x] `evals/harness/latency_collector.py`
- [x] `evals/graders/performance_grader.py`
- [x] `evals/tasks/performance/latency_benchmark.yaml`
- [x] `evals/tasks/performance/accuracy_benchmark.yaml`

---

### Phase 6: A/B æµ‹è¯•æ¡†æ¶ (P2) - âœ… å·²å®Œæˆ

#### äº¤ä»˜ç‰©

| æ–‡ä»¶ | è¯´æ˜ |
|-----|------|
| `evals/ab_testing/experiment.py` | å®éªŒå®šä¹‰ã€å˜ä½“ç®¡ç†ã€æµé‡åˆ†é… |
| `evals/ab_testing/analyzer.py` | ç»Ÿè®¡æ£€éªŒã€ç½®ä¿¡åŒºé—´ã€æ•ˆåº”é‡åˆ†æ |
| `evals/ab_testing/runner.py` | å®éªŒæ‰§è¡Œã€æ•°æ®æ”¶é›†ã€æŠ¥å‘Šç”Ÿæˆ |
| `evals/ab_testing/README.md` | å®Œæ•´ä½¿ç”¨æ–‡æ¡£ |

#### æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | è¯´æ˜ |
|-----|------|
| ABExperiment | å®éªŒå®šä¹‰ç±»ï¼Œæ”¯æŒå˜ä½“ã€æŒ‡æ ‡ã€åˆ†å±‚é…ç½® |
| ABTestAnalyzer | ç»Ÿè®¡åˆ†æå™¨ï¼Œæ”¯æŒ t-testã€æ¯”ä¾‹æ£€éªŒã€å¤šé‡æ¯”è¾ƒæ ¡æ­£ |
| ABTestRunner | å®éªŒè¿è¡Œå™¨ï¼Œåè°ƒæ‰§è¡Œã€æ”¶é›†æ•°æ®ã€ç”ŸæˆæŠ¥å‘Š |
| ExperimentRegistry | å®éªŒæ³¨å†Œè¡¨ï¼Œç®¡ç†å¤šä¸ªå®éªŒç”Ÿå‘½å‘¨æœŸ |

#### ç»Ÿè®¡åˆ†æåŠŸèƒ½

- Welch's t-test (å‡å€¼æ¯”è¾ƒ)
- z-test for proportions (æ¯”ä¾‹æ£€éªŒ)
- ç½®ä¿¡åŒºé—´è®¡ç®—
- æ•ˆåº”é‡è®¡ç®— (Cohen's d)
- ç»Ÿè®¡åŠŸæ•ˆåˆ†æ
- å¤šé‡æ¯”è¾ƒæ ¡æ­£ (Bonferroni, BH FDR)
- æ ·æœ¬é‡è®¡ç®—
- è¿è¡Œæ—¶é—´ä¼°ç®—

#### æµé‡åˆ†é…ç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ |
|-----|------|
| RANDOM | å®Œå…¨éšæœºåˆ†é… |
| USER_ID_HASH | åŸºäºç”¨æˆ·IDå“ˆå¸Œï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰ |
| SESSION_HASH | åŸºäºä¼šè¯å“ˆå¸Œ |
| DETERMINISTIC | ç¡®å®šæ€§åˆ†é…ï¼ˆæµ‹è¯•ç”¨ï¼‰ |

#### ä½¿ç”¨ç¤ºä¾‹

```python
from evals.ab_testing import ABExperiment, ABTestRunner, MetricDefinition

# åˆ›å»ºå®éªŒ
experiment = ABExperiment.create(
    name="åˆ†ç±»æ–¹æ³•å¯¹æ¯”",
    control_config={"method": "zero_shot"},
    treatment_config={"method": "few_shot"},
    primary_metric="intent_accuracy"
)

# æ·»åŠ æŠ¤æ æŒ‡æ ‡
experiment.guardrail_metrics = [
    MetricDefinition(name="error_rate", higher_is_better=False, is_guardrail=True)
]

# å¯åŠ¨å®éªŒ
experiment.start()

# è¿è¡Œå¹¶åˆ†æ
runner = ABTestRunner()
records = runner.run_batch(experiment, agent_func, test_cases)
analysis = runner.analyze_experiment(experiment)

# ç”ŸæˆæŠ¥å‘Š
report = runner.generate_report(analysis)
print(f"å»ºè®®: {analysis.recommendation}")
print(f"æ¨èé‡‡ç”¨: {analysis.winner}")
```

#### 6.1 å®éªŒæ¡†æ¶ï¼ˆå·²å®ç°ï¼‰

```python
# evals/ab_testing/experiment.py

@dataclass
class ABExperiment:
    id: str
    name: str
    description: str
    hypothesis: str

    variants: List[Variant]
    primary_metrics: List[MetricDefinition]
    secondary_metrics: List[MetricDefinition]
    guardrail_metrics: List[MetricDefinition]

    allocation_strategy: AllocationStrategy
    traffic_percentage: float
    stratification: List[StratificationRule]

    status: ExperimentStatus
    # ...
```

#### 6.2 äº¤ä»˜ç‰©
- [x] `evals/ab_testing/experiment.py`
- [x] `evals/ab_testing/analyzer.py`
- [x] `evals/ab_testing/runner.py`
- [x] `evals/ab_testing/README.md`
- [x] `evals/ab_testing/__init__.py`

---

## ä¸‰ã€ä¼˜å…ˆçº§æ’åº

| é˜¶æ®µ | å†…å®¹ | ä¼˜å…ˆçº§ | é¢„è®¡å·¥æ—¶ |
|-----|------|--------|---------|
| Phase 1 | ä¸šåŠ¡æŒ‡æ ‡ä½“ç³» | P0 | 3-5 å¤© |
| Phase 2 | ç”¨æˆ·æ¨¡æ‹Ÿå™¨ | P1 | 5-7 å¤© |
| Phase 3 | å¢å¼ºè¯„ä¼°ä»»åŠ¡ | P1 | 3-5 å¤© |
| Phase 4 | ä¸šåŠ¡çœ‹æ¿å¢å¼º | P1 | 3-5 å¤© |
| Phase 5 | æ€§èƒ½ä¸åŸºå‡† | P2 | 3-5 å¤© |
| Phase 6 | A/B æµ‹è¯•æ¡†æ¶ | P2 | 5-7 å¤© |

**æ€»è®¡ï¼š22-34 å¤©**

---

## å››ã€ç«‹å³å¯æ‰§è¡Œé¡¹

### ä»Šå¤©å°±èƒ½å¼€å§‹ï¼š

1. **åˆ›å»ºä¸šåŠ¡æŒ‡æ ‡å®šä¹‰æ–‡ä»¶**
   ```bash
   mkdir -p evals/metrics
   touch evals/metrics/business_metrics.yaml
   ```

2. **æ·»åŠ æ„å›¾è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹**
   - æ‰©å±• `evals/tasks/intent/` ä¸‹çš„æµ‹è¯•

3. **æ›´æ–°ç°æœ‰è¯„ä¼°æŠ¥å‘Šå¢åŠ ä¸šåŠ¡è§†è§’**
   - åœ¨ `reporter.py` ä¸­å¢åŠ ä¸šåŠ¡æŒ‡æ ‡è®¡ç®—

### éœ€è¦å›¢é˜Ÿè®¨è®ºï¼š

1. ä¸šåŠ¡æŒ‡æ ‡çš„ç›®æ ‡å€¼è®¾å®š
2. ç”¨æˆ·ç”»åƒåº“çš„å®Œå–„
3. A/B æµ‹è¯•çš„åˆ†æµç­–ç•¥
