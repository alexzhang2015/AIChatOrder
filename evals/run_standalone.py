#!/usr/bin/env python3
"""
ç‹¬ç«‹è¯„ä¼°è¿è¡Œå™¨

ä¸ä¾èµ–å®Œæ•´é¡¹ç›®ç¯å¢ƒï¼Œå¯å•ç‹¬è¿è¡Œè¯„ä¼°ä»»åŠ¡
ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•è¯„ä¼°æ¡†æ¶
"""

import sys
import json
import time
import yaml
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from evals.harness.models import (
    TaskConfig, TestCase, GraderConfig, GraderType,
    Trial, EvalResult, EvalSuiteResult, GraderResult, TaskCategory
)
from evals.graders.intent_grader import IntentGrader
from evals.graders.slot_grader import SlotGrader
from evals.graders.fuzzy_grader import FuzzyMatchGrader
from evals.graders.constraint_grader import ConstraintGrader
from evals.graders.transcript_grader import TranscriptGrader
from evals.harness.reporter import EvalReporter
from evals.harness.html_reporter import HTMLReporter


# ============ æ¨¡æ‹Ÿåˆ†ç±»å™¨ ============

class MockClassifier:
    """æ¨¡æ‹Ÿåˆ†ç±»å™¨ï¼Œç”¨äºæ¼”ç¤º"""

    INTENT_KEYWORDS = {
        "ORDER_NEW": ["æ¥ä¸€æ¯", "æˆ‘æƒ³ç‚¹", "ç»™æˆ‘", "è¦ä¸€æ¯", "ç‚¹ä¸€æ¯", "æ¥ä¸ª", "æ¥ä¸¤æ¯"],
        "ORDER_MODIFY": ["æ”¹æˆ", "æ¢æˆ", "åŠ ", "ä¸è¦", "å°‘", "å¤š"],
        "ORDER_CANCEL": ["å–æ¶ˆ", "ä¸è¦äº†", "ç®—äº†", "ä¸ç‚¹äº†", "ä¸æƒ³è¦"],
        "ORDER_QUERY": ["è®¢å•", "åˆ°å“ªäº†", "æŸ¥ä¸€ä¸‹", "çŠ¶æ€", "å‡†å¤‡å¥½"],
        "RECOMMEND": ["æ¨è", "å¥½å–", "ä»€ä¹ˆå¥½", "æ–°å“", "æœ€å¥½å–"],
        "PRODUCT_INFO": ["å¤šå°‘é’±", "ä»·æ ¼", "å¡è·¯é‡Œ", "çƒ­é‡", "é…æ–™"],
        "CHITCHAT": ["å¤©æ°”", "ä½ å¥½", "è°¢è°¢", "å¥½çš„", "å—¯"],
        "PAYMENT": ["ä»˜æ¬¾", "æ”¯ä»˜", "ä¹°å•", "ç»“è´¦"],
        "COMPLAINT": ["æŠ•è¯‰", "åšé”™", "ä¸å¯¹", "å¤ªæ…¢"],
    }

    SLOT_PATTERNS = {
        "product_name": {
            "æ‹¿é“": ["æ‹¿é“", "latte"],
            "ç¾å¼å’–å•¡": ["ç¾å¼", "americano"],
            "å¡å¸ƒå¥‡è¯º": ["å¡å¸ƒå¥‡è¯º", "cappuccino"],
            "æ‘©å¡": ["æ‘©å¡", "mocha"],
            "æ˜Ÿå†°ä¹": ["æ˜Ÿå†°ä¹", "frappuccino"],
            "é¦¥èŠ®ç™½": ["é¦¥èŠ®ç™½", "æ¾³ç™½", "flat white"],
            "å†·èƒå’–å•¡": ["å†·èƒ", "cold brew"],
            "ç„¦ç³–ç›å¥‡æœµ": ["ç„¦ç³–ç›å¥‡æœµ", "ç›å¥‡æœµ"],
        },
        "size": {
            "å°æ¯": ["å°æ¯", "tall"],
            "ä¸­æ¯": ["ä¸­æ¯", "grande"],
            "å¤§æ¯": ["å¤§æ¯", "venti"],
            "è¶…å¤§æ¯": ["è¶…å¤§æ¯", "trenta"],
        },
        "temperature": {
            "çƒ­": ["çƒ­", "hot"],
            "å†°": ["å†°", "iced", "å†·"],
            "æ¸©": ["æ¸©", "å¸¸æ¸©"],
        },
        "sweetness": {
            "æ— ç³–": ["æ— ç³–", "ä¸åŠ ç³–"],
            "å°‘ç³–": ["å°‘ç³–", "å¾®ç³–"],
            "åŠç³–": ["åŠç³–"],
            "æ ‡å‡†": ["æ ‡å‡†ç³–"],
            "å¤šç³–": ["å¤šç³–"],
        },
        "milk_type": {
            "ç‰›å¥¶": ["ç‰›å¥¶"],
            "ç‡•éº¦å¥¶": ["ç‡•éº¦å¥¶", "ç‡•éº¦"],
            "è±†å¥¶": ["è±†å¥¶", "è±†æµ†"],
            "æ¤°å¥¶": ["æ¤°å¥¶"],
        }
    }

    def classify(self, text: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ„å›¾åˆ†ç±»"""
        text_lower = text.lower()

        # è¯†åˆ«æ„å›¾
        intent = "UNKNOWN"
        confidence = 0.5

        for intent_type, keywords in self.INTENT_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    intent = intent_type
                    confidence = 0.85 + random.uniform(0, 0.1)
                    break
            if intent != "UNKNOWN":
                break

        # æå–æ§½ä½
        slots = {}
        for slot_name, values in self.SLOT_PATTERNS.items():
            for normalized, patterns in values.items():
                for pattern in patterns:
                    if pattern in text_lower or pattern in text:
                        slots[slot_name] = normalized
                        break

        # æå–æ•°é‡
        for i in range(1, 10):
            if f"{i}æ¯" in text or f"æ¥{i}" in text:
                slots["quantity"] = i
                break
        if "quantity" not in slots and ("ä¸€æ¯" in text or "æ¥ä¸€" in text or "æ¥ä¸ª" in text):
            slots["quantity"] = 1

        # æå–åŠ æ–™
        extras = []
        if "åŠ æµ“" in text or "æµ“ä¸€ç‚¹" in text:
            extras.append("åŠ æµ“")
        if "å¥¶æ²¹" in text:
            extras.append("å¥¶æ²¹")
        if extras:
            slots["extras"] = extras

        return {
            "intent": intent,
            "confidence": confidence,
            "slots": slots
        }

    def validate_constraints(self, slots: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿçº¦æŸéªŒè¯"""
        product = slots.get("product_name", "")
        temperature = slots.get("temperature", "")

        # å†°é¥®äº§å“çº¦æŸ
        cold_only = ["æ˜Ÿå†°ä¹", "å†·èƒå’–å•¡"]
        valid = True
        warnings = []
        auto_corrections = []
        adjusted_slots = slots.copy()

        if product in cold_only and temperature == "çƒ­":
            valid = False
            adjusted_slots["temperature"] = "å†°"
            warnings.append(f"{product}åªèƒ½åšå†°çš„")
            auto_corrections.append({
                "slot": "temperature",
                "from": "çƒ­",
                "to": "å†°",
                "message": f"{product}åªèƒ½åšå†°çš„ï¼Œå·²è‡ªåŠ¨è°ƒæ•´ä¸ºå†°çš„"
            })

        return {
            "valid": valid,
            "adjusted_slots": adjusted_slots,
            "warnings": warnings,
            "auto_corrections": auto_corrections
        }

    def match_fuzzy(self, text: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ¨¡ç³ŠåŒ¹é…"""
        fuzzy_patterns = {
            "ä¸è¦é‚£ä¹ˆç”œ|æ²¡é‚£ä¹ˆç”œ": ("sweetness", "åŠç³–", 0.85),
            "ç”œä¸€ç‚¹": ("sweetness", "å¤šç³–", 0.8),
            "å¥åº·ä¸€ç‚¹": ("sweetness", "æ— ç³–", 0.75),
            "æµ“ä¸€ç‚¹|åŠ æµ“": (None, None, 0.85, "add_extra_shot"),
            "ç»­å‘½æ°´": ("product_name", "ç¾å¼å’–å•¡", 0.9),
            "å¸¸æ¸©|ä¸è¦å¤ªçƒ«": ("temperature", "æ¸©", 0.8),
        }

        for pattern, result in fuzzy_patterns.items():
            import re
            if re.search(pattern, text):
                if len(result) == 3:
                    return {
                        "matched": True,
                        "slot_name": result[0],
                        "value": result[1],
                        "confidence": result[2],
                        "pattern": pattern,
                        "action": None,
                        "extra_mappings": {}
                    }
                else:
                    return {
                        "matched": True,
                        "slot_name": result[0],
                        "value": result[1],
                        "confidence": result[2],
                        "pattern": pattern,
                        "action": result[3],
                        "extra_mappings": {}
                    }

        return {"matched": False}


# ============ ç‹¬ç«‹è¯„ä¼°è¿è¡Œå™¨ ============

class StandaloneRunner:
    """ç‹¬ç«‹è¯„ä¼°è¿è¡Œå™¨"""

    def __init__(self, tasks_dir: str = "evals/tasks", results_dir: str = "evals/results"):
        self.tasks_dir = Path(tasks_dir)
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.classifier = MockClassifier()
        self.graders = {
            GraderType.INTENT_ACCURACY: IntentGrader,
            GraderType.SLOT_F1: SlotGrader,
            GraderType.FUZZY_MATCH: FuzzyMatchGrader,
            GraderType.CONSTRAINT_VALIDATION: ConstraintGrader,
            GraderType.TRANSCRIPT: TranscriptGrader,
        }

    def load_task(self, task_path: str) -> TaskConfig:
        """åŠ è½½ä»»åŠ¡"""
        path = Path(task_path)
        # å¦‚æœè·¯å¾„å·²ç»æ˜¯ç»å¯¹è·¯å¾„æˆ–å·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨
        if path.is_absolute() or path.exists():
            pass
        elif not str(task_path).startswith(str(self.tasks_dir)):
            path = self.tasks_dir / path

        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)

        return TaskConfig.from_dict(data)

    def load_suite(self, suite_name: str) -> List[TaskConfig]:
        """åŠ è½½å¥—ä»¶"""
        tasks = []
        for task_file in self.tasks_dir.rglob("*.yaml"):
            try:
                # ç›´æ¥ç”¨ç»å¯¹è·¯å¾„è¯»å–
                with open(task_file, "r", encoding="utf-8") as f:
                    data = yaml.safe_load(f)
                task = TaskConfig.from_dict(data)

                if suite_name == "all":
                    tasks.append(task)
                elif suite_name == "regression" and task.category == TaskCategory.REGRESSION:
                    tasks.append(task)
                elif suite_name == "capability" and task.category == TaskCategory.CAPABILITY:
                    tasks.append(task)
                elif suite_name == "edge_case" and task.category == TaskCategory.EDGE_CASE:
                    tasks.append(task)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ä»»åŠ¡å¤±è´¥ {task_file}: {e}")
        return tasks

    def run_task(self, task: TaskConfig, num_trials: int = 3) -> EvalResult:
        """è¿è¡Œå•ä¸ªä»»åŠ¡"""
        print(f"\nğŸ“ è¿è¡Œä»»åŠ¡: {task.id} - {task.name}")
        trials = []

        for i in range(num_trials):
            trial = self._run_trial(task, i)
            trials.append(trial)
            status = "âœ…" if trial.passed else "âŒ"
            print(f"   Trial {i+1}: {status} ({trial.duration_ms:.0f}ms)")

        # è®¡ç®—ç»“æœ
        passes = [t.passed for t in trials]
        result = EvalResult(
            task_id=task.id,
            task_name=task.name,
            category=task.category,
            trials=trials,
            pass_at_1=1.0 if passes[0] else 0.0,
            pass_at_k=1.0 if any(passes) else 0.0,
            pass_all=1.0 if all(passes) else 0.0,
            completed_at=datetime.now().isoformat()
        )

        status = "âœ… PASS" if result.pass_all >= 1.0 else "âŒ FAIL"
        print(f"   ç»“æœ: {status} (pass@1={result.pass_at_1:.0%}, pass_all={result.pass_all:.0%})")

        return result

    def _run_trial(self, task: TaskConfig, trial_num: int) -> Trial:
        """è¿è¡Œå•æ¬¡è¯•éªŒ"""
        start_time = time.time()

        predictions = []
        grader_results = {}
        all_passed = True

        # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        for test_case in task.test_cases:
            if isinstance(test_case.input, str):
                # æ–‡æœ¬è¾“å…¥ - æ„å›¾åˆ†ç±»
                result = self.classifier.classify(test_case.input)
                predictions.append({
                    "input": test_case.input,
                    "intent": result["intent"],
                    "confidence": result["confidence"],
                    "slots": result["slots"]
                })
            elif isinstance(test_case.input, dict):
                # ç»“æ„åŒ–è¾“å…¥ - çº¦æŸéªŒè¯
                result = self.classifier.validate_constraints(test_case.input)
                predictions.append({
                    "input": test_case.input,
                    **result
                })

        # è¿è¡Œè¯„åˆ†å™¨
        for grader_config in task.graders:
            grader_class = self.graders.get(grader_config.type)
            if grader_class:
                grader = grader_class(config=grader_config)
                result = grader.evaluate(predictions, task.test_cases)
                grader_results[grader_config.type.value] = result
                if not result.passed and grader_config.required:
                    all_passed = False

        duration_ms = (time.time() - start_time) * 1000

        return Trial(
            task_id=task.id,
            trial_number=trial_num,
            outcome={"predictions": predictions},
            grader_results=grader_results,
            passed=all_passed,
            duration_ms=duration_ms
        )

    def run_suite(self, suite_name: str, num_trials: int = 3) -> EvalSuiteResult:
        """è¿è¡Œå¥—ä»¶"""
        tasks = self.load_suite(suite_name)
        print(f"\nğŸš€ è¿è¡Œè¯„ä¼°å¥—ä»¶: {suite_name}")
        print(f"   ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"   è¯•éªŒæ¬¡æ•°: {num_trials}")

        results = []
        started_at = datetime.now()

        for task in tasks:
            result = self.run_task(task, num_trials)
            results.append(result)

        # ç»Ÿè®¡
        tasks_passed = sum(1 for r in results if r.pass_all >= 1.0)
        tasks_failed = len(results) - tasks_passed
        overall_pass_rate = tasks_passed / len(results) if results else 0

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for category in TaskCategory:
            cat_results = [r for r in results if r.category == category]
            if cat_results:
                cat_passed = sum(1 for r in cat_results if r.pass_all >= 1.0)
                by_category[category.value] = {
                    "total": len(cat_results),
                    "passed": cat_passed,
                    "pass_rate": cat_passed / len(cat_results)
                }

        completed_at = datetime.now()
        total_duration = (completed_at - started_at).total_seconds() * 1000

        return EvalSuiteResult(
            suite_name=suite_name,
            tasks_total=len(results),
            tasks_passed=tasks_passed,
            tasks_failed=tasks_failed,
            overall_pass_rate=overall_pass_rate,
            results=results,
            by_category=by_category,
            started_at=started_at.isoformat(),
            completed_at=completed_at.isoformat(),
            total_duration_ms=total_duration
        )


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ç‹¬ç«‹è¯„ä¼°è¿è¡Œå™¨")
    parser.add_argument("--suite", default="all", choices=["all", "regression", "capability", "edge_case"])
    parser.add_argument("--trials", type=int, default=3)
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--report", default="all", choices=["none", "json", "markdown", "html", "all"])

    args = parser.parse_args()

    # è¿è¡Œè¯„ä¼°
    runner = StandaloneRunner()
    result = runner.run_suite(args.suite, args.trials)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print(f"ğŸ“Š è¯„ä¼°æ‘˜è¦: {result.suite_name}")
    print("=" * 60)
    print(f"æ€»ä»»åŠ¡æ•°: {result.tasks_total}")
    print(f"é€šè¿‡: {result.tasks_passed}")
    print(f"å¤±è´¥: {result.tasks_failed}")
    print(f"é€šè¿‡ç‡: {result.overall_pass_rate:.1%}")
    print(f"æ€»è€—æ—¶: {result.total_duration_ms/1000:.2f}s")

    if result.by_category:
        print("\næŒ‰ç±»åˆ«:")
        for cat, stats in result.by_category.items():
            print(f"  {cat}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.0%})")

    # ç”ŸæˆæŠ¥å‘Š
    if args.report != "none":
        print("\nğŸ“„ ç”ŸæˆæŠ¥å‘Š...")

        if args.report in ["json", "all"]:
            reporter = EvalReporter()
            json_path = reporter.generate_json_report(result)
            print(f"   JSON: {json_path}")

        if args.report in ["markdown", "all"]:
            reporter = EvalReporter()
            md_path = reporter.generate_markdown_report(result)
            print(f"   Markdown: {md_path}")

        if args.report in ["html", "all"]:
            html_reporter = HTMLReporter()
            html_path = html_reporter.generate_suite_report(result)
            print(f"   HTML: {html_path}")

    print("\nâœ… å®Œæˆ!")


if __name__ == "__main__":
    main()
